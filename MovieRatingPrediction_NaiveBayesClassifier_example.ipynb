{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try with an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "x = [\n",
    "    \"This was an awesome movie\" , \n",
    "    \"Great movie! , I liked it a lot\" ,\n",
    "    \"Happy ending! Awesome acting by the hero\" ,\n",
    "    \"Loved it , truly great\" ,\n",
    "    \"bad not upto the mark\" ,\n",
    "    \"could have been better\" ,\n",
    "    \"Surely a disappointing movie\"\n",
    "]\n",
    "\n",
    "y = [1,1,1,1,0,0,0]\n",
    "\n",
    "xtest = [\n",
    "    \"I was happy and I loved the acting in the movie\" ,\n",
    "    \"The movie is bad\"\n",
    "]\n",
    "\n",
    "print(type(x[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.snowball import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def getStemmedReview(review):\n",
    "    \n",
    "    review = review.replace('<br /><br />',\" \")\n",
    "    \n",
    "    # Step 1 : Tokenize\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    wordsList = tokenizer.tokenize(review)\n",
    "    wordsList = [word.lower() for word in wordsList]\n",
    "    \n",
    "    \n",
    "    # Step 2 : Remove Stop Words\n",
    "    sw = stopwords.words('english')\n",
    "    sw = set(sw)\n",
    "    wordsList = [word for word in wordsList if word not in sw]\n",
    "    \n",
    "    \n",
    "    # Step 3 : Stemming\n",
    "    ps = PorterStemmer()\n",
    "    wordsList = [ps.stem(word) for word in wordsList]\n",
    "    #print(wordsList)\n",
    "    \n",
    "    # return as a sentence\n",
    "    cleaned_review = \" \".join(wordsList)\n",
    "    \n",
    "    return cleaned_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'awesom movi'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getStemmedReview(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "xclean = [getStemmedReview(i) for i in x]\n",
    "xtest_clean = [getStemmedReview(i) for i in xtest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['awesom movi', 'great movi like lot', 'happi end awesom act hero', 'love truli great', 'bad upto mark', 'could better', 'sure disappoint movi']\n"
     ]
    }
   ],
   "source": [
    "print(xclean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(ngram_range=(1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
      "  0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 1 1 1 0 0 0 0 1 1 1 0 0\n",
      "  0 0 0 0 0]\n",
      " [1 1 1 1 1 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0\n",
      "  0 1 1 0 0]\n",
      " [0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      "  0 0 0 1 1]\n",
      " [0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1\n",
      "  1 0 0 0 0]]\n",
      "\n",
      "(7, 41)\n",
      "\n",
      "{'awesom': 2, 'movi': 31, 'awesom movi': 5, 'great': 17, 'like': 24, 'lot': 26, 'great movi': 18, 'movi like': 32, 'like lot': 25, 'great movi like': 19, 'movi like lot': 33, 'happi': 20, 'end': 14, 'act': 0, 'hero': 23, 'happi end': 21, 'end awesom': 15, 'awesom act': 3, 'act hero': 1, 'happi end awesom': 22, 'end awesom act': 16, 'awesom act hero': 4, 'love': 27, 'truli': 37, 'love truli': 28, 'truli great': 38, 'love truli great': 29, 'bad': 6, 'upto': 39, 'mark': 30, 'bad upto': 7, 'upto mark': 40, 'bad upto mark': 8, 'could': 10, 'better': 9, 'could better': 11, 'sure': 34, 'disappoint': 12, 'sure disappoint': 35, 'disappoint movi': 13, 'sure disappoint movi': 36}\n"
     ]
    }
   ],
   "source": [
    "x_vectorizedCorpus = cv.fit_transform(xclean).toarray()\n",
    "print(x_vectorizedCorpus)\n",
    "\n",
    "print()\n",
    "print(x_vectorizedCorpus.shape)\n",
    "print()\n",
    "print(cv.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0\n",
      "  0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
      "  0 0 0 0 0]]\n",
      "{'awesom': 2, 'movi': 31, 'awesom movi': 5, 'great': 17, 'like': 24, 'lot': 26, 'great movi': 18, 'movi like': 32, 'like lot': 25, 'great movi like': 19, 'movi like lot': 33, 'happi': 20, 'end': 14, 'act': 0, 'hero': 23, 'happi end': 21, 'end awesom': 15, 'awesom act': 3, 'act hero': 1, 'happi end awesom': 22, 'end awesom act': 16, 'awesom act hero': 4, 'love': 27, 'truli': 37, 'love truli': 28, 'truli great': 38, 'love truli great': 29, 'bad': 6, 'upto': 39, 'mark': 30, 'bad upto': 7, 'upto mark': 40, 'bad upto mark': 8, 'could': 10, 'better': 9, 'could better': 11, 'sure': 34, 'disappoint': 12, 'sure disappoint': 35, 'disappoint movi': 13, 'sure disappoint movi': 36}\n",
      "\n",
      "(2, 41)\n"
     ]
    }
   ],
   "source": [
    "# test set\n",
    "xtest_vectorizedCorpus = cv.transform(xtest_clean).toarray()\n",
    "print(xtest_vectorizedCorpus)\n",
    "print(cv.vocabulary_)\n",
    "print()\n",
    "print(xtest_vectorizedCorpus.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Multinomial Event Naive bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform training.\n",
    "mnb.fit(x_vectorizedCorpus, y )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb.predict(xtest_vectorizedCorpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.13904125, 0.86095875],\n",
       "       [0.61648526, 0.38351474]])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb.predict_proba(xtest_vectorizedCorpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(mnb.score(x_vectorizedCorpus , y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate Bernaulli Event Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb = BernoulliNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)\n"
     ]
    }
   ],
   "source": [
    "print(bnb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)\n"
     ]
    }
   ],
   "source": [
    "print(bnb.fit(x_vectorizedCorpus , y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.13312578 0.86687422]\n",
      " [0.80373674 0.19626326]]\n"
     ]
    }
   ],
   "source": [
    "print(bnb.predict_proba(xtest_vectorizedCorpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0]\n"
     ]
    }
   ],
   "source": [
    "print(bnb.predict(xtest_vectorizedCorpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(bnb.score(x_vectorizedCorpus , y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
